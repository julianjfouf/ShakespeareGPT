{"cells":[{"cell_type":"code","execution_count":81,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-12-28T23:52:56.330743Z","iopub.status.busy":"2023-12-28T23:52:56.330335Z","iopub.status.idle":"2023-12-28T23:52:56.341480Z","shell.execute_reply":"2023-12-28T23:52:56.340399Z","shell.execute_reply.started":"2023-12-28T23:52:56.330709Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/shakespeare/input.txt\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"markdown","metadata":{},"source":["<h1>First, we will explore the bigram model for this and test its capabilities. We expect it to be ineffective in generating reasonable results because it is only given the previous character for context which is not nearly enough to be coherent</h1>"]},{"cell_type":"code","execution_count":82,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:57.319816Z","iopub.status.busy":"2023-12-28T23:52:57.319439Z","iopub.status.idle":"2023-12-28T23:52:57.326780Z","shell.execute_reply":"2023-12-28T23:52:57.325675Z","shell.execute_reply.started":"2023-12-28T23:52:57.319787Z"},"trusted":true},"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x7d6166b574d0>"]},"execution_count":82,"metadata":{},"output_type":"execute_result"}],"source":["# import libraries\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","# seed the random generators for reproducible results\n","torch.manual_seed(1337)"]},{"cell_type":"code","execution_count":83,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:57.644515Z","iopub.status.busy":"2023-12-28T23:52:57.643527Z","iopub.status.idle":"2023-12-28T23:52:57.651191Z","shell.execute_reply":"2023-12-28T23:52:57.650095Z","shell.execute_reply.started":"2023-12-28T23:52:57.644479Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# hyperparameters of model\n","\n","batch_size = 32 # independent sequences of the data to process in parallel\n","block_size = 8 # maximum context length for predictions (in bigram model, it only uses the previous character and does not take any thing from more previous characters so this does not do anything)\n","max_iters = 3000 # iterations of training\n","eval_interval = 300 # how often to report the loss of the model in training\n","learning_rate = 1e-2 # how large of a correction the model will make to itself after a training example\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # train on gpu if available otherwise cpu\n","eval_iters = 200 # idk yet LOL!!!!!\n","\n","print(device)"]},{"cell_type":"code","execution_count":84,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:58.160137Z","iopub.status.busy":"2023-12-28T23:52:58.159751Z","iopub.status.idle":"2023-12-28T23:52:58.186856Z","shell.execute_reply":"2023-12-28T23:52:58.185910Z","shell.execute_reply.started":"2023-12-28T23:52:58.160104Z"},"trusted":true},"outputs":[],"source":["# first open the input file in read mode and encode it using utf-8 and read in all the text into variable, text\n","with open('/kaggle/input/shakespeare/input.txt', 'r', encoding = 'utf-8') as f:\n","    text = f.read()\n","    \n","# first turn text into a set to remove duplicate characters, then into a list, then sort the list\n","chars = sorted(list(set(text))) # this tells us the vocabulary needed by the network to reproduce shakespeare according to our input training text\n","\n","# as mentioned chars is the vocabulary needed to reproduce and its length is our vocab_size\n","vocab_size = len(chars)\n","\n","# first enumerate chars so that it is numbers, but this also returns the original version as well so we take numbers and chars, and we make a dictionary of mappings from chars to numbers\n","stoi = {ch: i for i, ch in enumerate(chars)}\n","\n","# same as above but reverse\n","itos = {i: ch for i, ch in enumerate(chars)}\n","\n","# now we need an encoder and decoder to convert between strings and integers for us\n","\n","# lambda means this is a function, encode is a function that takes s (string) as an input and returns a list of integers that are mapped from each character in the string\n","encode = lambda s: [stoi[c] for c in s]\n","\n","# function that takes a list of integers and converts to a list of the corresponding character mappings, then joins them into one string\n","decode = lambda l: ''.join([itos[i] for i in l])"]},{"cell_type":"code","execution_count":85,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:58.556243Z","iopub.status.busy":"2023-12-28T23:52:58.555382Z","iopub.status.idle":"2023-12-28T23:52:58.815188Z","shell.execute_reply":"2023-12-28T23:52:58.813904Z","shell.execute_reply.started":"2023-12-28T23:52:58.556210Z"},"trusted":true},"outputs":[],"source":["# data preparation\n","\n","# the data will be a tensor of the encoded text (a long list of integer mappings of the original text), as the datatype of longs because all are integers\n","data = torch.tensor(encode(text), dtype = torch.long)\n","\n","# we want a train/val split of 90/10\n","\n","# this is a marker at the 90% of the dataset\n","n = int(0.9 * len(data))\n","\n","# first 90%\n","train_data = data[:n]\n","\n","# last 10%\n","val_data = data[n:]"]},{"cell_type":"code","execution_count":86,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:58.884110Z","iopub.status.busy":"2023-12-28T23:52:58.883254Z","iopub.status.idle":"2023-12-28T23:52:58.891234Z","shell.execute_reply":"2023-12-28T23:52:58.890203Z","shell.execute_reply.started":"2023-12-28T23:52:58.884075Z"},"trusted":true},"outputs":[],"source":["# data loading\n","def get_batch(split):\n","    \n","    # if we want training split give training data otherwise val data\n","    data = train_data if split == 'train' else val_data\n","    \n","    # generate 32 random integers between 0 and length of data - block_size since we take a consecutive chunk of block_size for the batch\n","    ix = torch.randint(0, len(data) - block_size, (batch_size,))\n","    \n","    # make a vertical stack (2D array basically). for each index in the 32 random indices, we want to pull from the data all the data at the index to the index + block_size to get block_size amount of data\n","    x = torch.stack([data[i: i + block_size] for i in ix])\n","    \n","    # for each index in the 32 random indices, we want to pull from the data all the data at the index + 1 (because we are always predicting the next character) to index + block_size + 1\n","    y = torch.stack([data[i + 1: i + block_size + 1] for i in ix])\n","    \n","    # we want to put these batches on the device that we wish to train on\n","    x, y = x.to(device), y.to(device)\n","    \n","    # we give the data batches back\n","    return x, y"]},{"cell_type":"code","execution_count":87,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:59.253321Z","iopub.status.busy":"2023-12-28T23:52:59.252917Z","iopub.status.idle":"2023-12-28T23:52:59.260262Z","shell.execute_reply":"2023-12-28T23:52:59.259237Z","shell.execute_reply.started":"2023-12-28T23:52:59.253291Z"},"trusted":true},"outputs":[],"source":["@torch.no_grad() # decorator to disable gradient tracking\n","\n","def estimate_loss():\n","    \n","    out = {}\n","    \n","    # set the model to evaluation mode\n","    model.eval() # model is a global variable in this example so this works, but better practice to pass in as a function parameter\n","    \n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        \n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X, Y)\n","            losses[k] = loss.item()\n","        \n","        out[split] = losses.mean()\n","        \n","    # set the model to training mode\n","    model.train()\n","    \n","    return out"]},{"cell_type":"code","execution_count":88,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:59.521572Z","iopub.status.busy":"2023-12-28T23:52:59.521152Z","iopub.status.idle":"2023-12-28T23:52:59.533363Z","shell.execute_reply":"2023-12-28T23:52:59.532417Z","shell.execute_reply.started":"2023-12-28T23:52:59.521537Z"},"trusted":true},"outputs":[],"source":["# simplest bigram model of language model\n","\n","class BigramLanguageModel(nn.Module):\n","    \n","    # on initialization\n","    def __init__(self, vocab_size):\n","        \n","        # call the initialization of the parent class\n","        super().__init__()\n","        \n","        # the token embedding table of this model will be an embedding lookup table of vocab_size by vocab_size because for each previous character (vocab_size possibilities), we want to generate a probability distribution of the next character (vocab_size possibilities)\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n","        \n","    # on feed forward data\n","    def forward(self, idx, targets = None):\n","        \n","        # we pull out the probability distribution created by the model at the indexes we are interested in making predictions on\n","        logits = self.token_embedding_table(idx)\n","        \n","        # if no targets are passed we can't really do any loss calculation of the model's efficacy because we have nothing to validate its results\n","        if targets is None:\n","            \n","            # so the loss is none\n","            loss = None\n","            \n","        # however if we do have targets to compare to\n","        else:\n","            \n","            # we want to get the three dimensions of our probability distribution logits (Bytes (# of batches), Time (location within the batch, we go from 1 character to block_size characters and we take this to be traveling through time), Channels (each instance in time has a probability distribution that determines what the next instance of time will look like (what the next character will be)))\n","            B, T, C = logits.shape\n","            \n","            # in order to comply with the requirements of the cross entropy loss function we need to compress our logits and targets\n","            \n","            # we will merge the batches and time dimensions together but maintain the channels\n","            logits = logits.view(B * T, C)\n","            \n","            # we will correspondingly merge the batches and time dimensions of the target\n","            targets = targets.view(B * T)\n","            \n","            # then we can get the loss of the logits based on what the labels are\n","            loss = F.cross_entropy(logits, targets)\n","            \n","        # then we return the logits and loss generated by the model\n","        return logits, loss\n","    \n","    # generating predictions from the model\n","    def generate(self, idx, max_new_tokens):\n","        \n","        # each token is a character prediction by the model so we want to produce as much content as requested\n","        for _ in range(max_new_tokens):\n","            \n","            # idx is the input given to the model so we pass this starting input into the model and it generates the next character\n","            logits, loss = self(idx)\n","            \n","            # then we are only interested in the last time step in the model's sequence of logits production so we index into the last instance in time in the generation process\n","            logits = logits[:, -1, :] # becomes (B, C)\n","            \n","            # then we apply a softmax on the logits by exponentiating them and normalizing to get a probability distribution and we want to do this for the last dimension of logits because this will always be the final result of model in which we are interested in\n","            probs = F.softmax(logits, dim = - 1)\n","            \n","            # then using this probability distribution generated by the model, we want to randomly sample the next character in the sequence\n","            idx_next = torch.multinomial(probs, num_samples = 1)\n","            \n","            # then we will add this new character index to our input and restart the process of adding more characters and producing a max_new_tokens sized output\n","            idx = torch.cat((idx, idx_next), dim = 1)\n","            \n","        # then finally we return the production of the model\n","        return idx"]},{"cell_type":"code","execution_count":89,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:52:59.884519Z","iopub.status.busy":"2023-12-28T23:52:59.884107Z","iopub.status.idle":"2023-12-28T23:52:59.891131Z","shell.execute_reply":"2023-12-28T23:52:59.890014Z","shell.execute_reply.started":"2023-12-28T23:52:59.884487Z"},"trusted":true},"outputs":[],"source":["# model initialization\n","\n","# bigram model with its vocab size\n","model = BigramLanguageModel(vocab_size)\n","\n","# move the model to the device we wish to train on\n","model = model.to(device)\n","\n","# then we choose which optimizers we want to use for training and passing in the model parameters to train and the learning rate of training\n","optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"]},{"cell_type":"code","execution_count":90,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:53:00.597816Z","iopub.status.busy":"2023-12-28T23:53:00.597122Z","iopub.status.idle":"2023-12-28T23:53:11.253191Z","shell.execute_reply":"2023-12-28T23:53:11.252187Z","shell.execute_reply.started":"2023-12-28T23:53:00.597781Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.7305, val loss 4.7241\n","step 300: train loss 2.8110, val loss 2.8249\n","step 600: train loss 2.5434, val loss 2.5682\n","step 900: train loss 2.4932, val loss 2.5088\n","step 1200: train loss 2.4863, val loss 2.5035\n","step 1500: train loss 2.4665, val loss 2.4921\n","step 1800: train loss 2.4683, val loss 2.4936\n","step 2100: train loss 2.4696, val loss 2.4846\n","step 2400: train loss 2.4638, val loss 2.4879\n","step 2700: train loss 2.4738, val loss 2.4911\n"]}],"source":["# training time\n","\n","# for each iteration of training\n","for iter in range(max_iters):\n","    \n","    # if the iteration is one that falls on the evaluation of training interval\n","    if iter % eval_interval == 0:\n","        \n","        # we want to get the current loss on the validation and training set\n","        losses = estimate_loss()\n","        \n","        # then we want to display the current training and validation loss at this iteration\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","    # we get a batch to train on from the training dataset\n","    xb, yb = get_batch('train')\n","    \n","    # we get logits from the model on this batch\n","    logits, loss = model(xb, yb)\n","    \n","    # we reset the gradients on the optimizer and set them to None because this is most efficient\n","    optimizer.zero_grad(set_to_none = True)\n","    \n","    # then we back propagate and calculate gradients for the parameters to descend the loss function\n","    loss.backward()\n","    \n","    # then we take a step in the right direction for the model parameters\n","    optimizer.step()"]},{"cell_type":"code","execution_count":91,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:53:13.852391Z","iopub.status.busy":"2023-12-28T23:53:13.851590Z","iopub.status.idle":"2023-12-28T23:53:13.998609Z","shell.execute_reply":"2023-12-28T23:53:13.997652Z","shell.execute_reply.started":"2023-12-28T23:53:13.852359Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","\n","CEThik brid owindakis b, bth\n","\n","HAPet bobe d e.\n","S:\n","O:3 my d?\n","LUCous:\n","Wanthar u qur, t.\n","War dXENDoate awice my.\n","\n","Hastarom oroup\n","Yowhthetof isth ble mil ndill, ath iree sengmin lat Heriliovets, and Win nghir.\n","Swanousel lind me l.\n","HAshe ce hiry:\n","Supr aisspllw y.\n","Hentofu n Boopetelaves\n","MPOLI s, d mothakleo Windo whth eisbyo the m dourive we higend t so mower; te\n","\n","AN ad nterupt f s ar igr t m:\n","\n","Thin maleronth,\n","Mad\n","RD:\n","\n","WISo myrangoube!\n","KENob&y, wardsal thes ghesthinin couk ay aney IOUSts I&fr y ce.\n","J\n"]}],"source":["# the input we want to give to the model is just a 1 by 1 matrix that is initially just a 0\n","# i.e.\n","#   C0\n","# R0 0\n","\n","# this context will be dataype long and want to move it to the device we wish to work on\n","context = torch.zeros((1, 1), dtype = torch.long, device = device)\n","\n","# then we want to generate 500 characters given the context using the model specifically on the device we want to work on and then we want to pull out the indexes generated, convert them to a list of integers, and then give them to the decoder to decode so we can print them\n","print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))"]},{"cell_type":"markdown","metadata":{},"source":["<h1>Now we will investiage the much more effective transformer model approach to llm which thrives on self-attention and allowing the characters to work with each other to generate predictions rather than being limited to only the previous character to be the only thing the network can use to predict the next character</h1>"]},{"cell_type":"code","execution_count":92,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:53.809863Z","iopub.status.busy":"2023-12-28T23:55:53.809131Z","iopub.status.idle":"2023-12-28T23:55:53.816554Z","shell.execute_reply":"2023-12-28T23:55:53.815514Z","shell.execute_reply.started":"2023-12-28T23:55:53.809810Z"},"trusted":true},"outputs":[],"source":["# hyperparameters of the transformer model\n","batch_size = 64 # how many independent sequences to train on in parallel\n","block_size = 256 # maximum size of input context and previous characters the network can use to predict the next character\n","max_iters = 5000 # how many iterations of training\n","eval_interval = 500 # how often to display the current loss in training\n","learning_rate = 3e-4 # the rate of parameter updating\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # deciding which device to train on\n","eval_iters = 200\n","n_embed = 384 # how many dimensions the network has to work with for embedding the vocabulary\n","n_head = 6 # the number of heads to use to extract features from the input data in order to better be able to understand what came before in order to more effectively predict what should come next\n","n_layer = 6 # the number of linear layers applied throughout the model to help the model not only transform the input into something it can understand, but actually be able to let the information marinate and go through some internal processing before the output\n","dropout = 0.2 # the proportion of neurons to shut off randomly during each iteration in order to prevent overtraining and overfitting to the dataset to make sure that the network is actually able to perform and predict well and not just simply memorize the data"]},{"cell_type":"code","execution_count":93,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:54.314265Z","iopub.status.busy":"2023-12-28T23:55:54.313905Z","iopub.status.idle":"2023-12-28T23:55:54.327726Z","shell.execute_reply":"2023-12-28T23:55:54.326493Z","shell.execute_reply.started":"2023-12-28T23:55:54.314235Z"},"trusted":true},"outputs":[],"source":["# definition of a head to extract features from the input using self-attention\n","\n","class Head(nn.Module):\n","    \"\"\" one head of self-attention \"\"\"\n","    \n","    # on initialization\n","    def __init__(self, head_size):\n","        \n","        # call the parent function initialization\n","        super().__init__()\n","        \n","        # the key attribute will be a linear layer of input n_embed and output head_size with no bias\n","        self.key = nn.Linear(n_embed, head_size, bias = False)\n","        \n","        # the query attribute will be a linear layer of input n_embed and output head_size with no bias\n","        self.query = nn.Linear(n_embed, head_size, bias = False)\n","        \n","        # the value attribute will be a linear layer of input n_embed and output head_size with no bias\n","        self.value = nn.Linear(n_embed, head_size, bias = False)\n","        \n","        # we will register a buffer called 'tril' which will be a tril applied to a block_size by block_size tensor of ones\n","        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n","        \n","        # i.e.\n","        # 1 1 1     1 0 0\n","        # 1 1 1 --> 1 1 0\n","        # 1 1 1     1 1 1\n","        \n","        # this is because when doing matrix multiplication like this and normalizing each row, you will be able to get a cumulative average as you progress through the rows which is what we want\n","        # the idea is that the prediction for the next character will take into account the previous characters\n","        # a primitive way of doing this is to simply just take the average of the previous characters and use this to predict the next character\n","        # however, a more effective and precise way, is to allow the model to assign its own weights and values to the previous characters based on what they are and their effect on what the next character might be\n","        \n","        # on the level of words, lets say that we are writing a sentence and all the previous words in the sentence where we want to predict the next word are all nouns and adjectives.\n","        # well in order for this to be a sentence we must have a verb, so the model is very likely to predict a verb as the next word in the sequence if it looks at the previous words and sees that they are all nouns and adjectives.\n","        # it will choose which verb based on its own feelings about which words that have come before are most interesting and their weights on what the predicted word will be\n","        \n","        # a simple example:\n","        # context a disabled man ...\n","        # the model will look at the past and see oh, there are no verbs so the next one could be a verb, and then it takes special note of the adjective describing the man as disabled and then reduces the likelihood that the verb has to do with physical action\n","        # in the primitive averaging the past words approach, the model may have missed out on this key detail that the man is disabled because an average is a lossy conversion and this important information may have been lost in the average of the past words\n","        # this is especially true when there are many past words in the hundreds and it becomes much better if the model is able to pick and choose which words stand out the most to it and be able to predict the next word based on its own accord\n","        \n","        # then the dropout attribute will be a dropout layer that drops 20% of the weights\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    # then we feed forward the input\n","    def forward(self, x):\n","        \n","        # we get the B, T, C dimensions of the input\n","        B, T, C = x.shape\n","        \n","        # we get the keys for the given input\n","        k = self.key(x) # B, T, head_size\n","        \n","        # we get the queries for the given input\n","        q = self.query(x) # B, T, head_size\n","        \n","        # then we get our weights by matrix multiplying the queries by the transpose of the keys along the first and second dimension, and then we kaiming init/normalize by dividing by the square root of the head_size\n","        weights = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5 # (B, T, head_size) @ (B, head_size, T )-> (B, T, T)\n","        \n","        # then we apply the tril mask by setting anything in the weights that would be 0 on the tril matrix to -infinity\n","        weights = weights.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n","        \n","        # then we apply a softmax along the last dimension of the weights so that we can exponentiate the logits and normalize them to get some type of probability distribution \n","        weights = F.softmax(weights, dim = -1) # (B, T, T)\n","        \n","        # then we randomly dropout some of the weights\n","        weights = self.dropout(weights)\n","        \n","        # then we apply the values onto the input \n","        v = self.value(x) # (B, T, head_size)\n","        \n","        # then depending on how interested the model is in the values at certain locations, it will extract more of those values and take them into more consideration when making the prediction\n","        out = weights @ v # (B, T, T) @ (B, T, head_size) \n","        \n","        # the keys and queries go hand in hand.\n","        # the model makes a query and gets the resulting keys for the queries\n","        # for characters that the model finds to stand out and be more interesting, it will extract more value out of that character\n","        # for characters that the model does not care about, it will extract little value out of the character\n","        \n","        # in the \"a disbaled man\" example at the word levels\n","        \n","        # 0.2 0 0\n","        # 0.3 0.7 0      -> multiplying this by another matrix will pull out the most from disabled meaning that the model has taken most interest in this adjective and will use a good portion of this word in order to make its prediction\n","        # 0.1 0.7 0.2\n","        \n","        # then the model will return its output from the transforming head\n","        return out"]},{"cell_type":"code","execution_count":94,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:55.015284Z","iopub.status.busy":"2023-12-28T23:55:55.014887Z","iopub.status.idle":"2023-12-28T23:55:55.023457Z","shell.execute_reply":"2023-12-28T23:55:55.022485Z","shell.execute_reply.started":"2023-12-28T23:55:55.015253Z"},"trusted":true},"outputs":[],"source":["# multiple heads used in parallel\n","\n","class MultiHeadAttention(nn.Module):\n","    \"\"\" multiple heads of self attention in parallel\"\"\"\n","    \n","    # on initialization\n","    def __init__(self, num_heads, head_size):\n","        \n","        # call the parent function intialization\n","        super().__init__()\n","        \n","        # the heads attribute will be a list of modules. this list of modules will be num_heads of heads with each Head having head_size\n","        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n","        \n","        # the proj attribute will be for when the model will project and convert the head_size * num_heads input into n_embed output\n","        self.proj = nn.Linear(head_size * num_heads, n_embed)\n","        \n","        # then the dropout attribute will be a dropoutlayer that drops out 20% of the weights\n","        self.dropout = nn.Dropout(dropout)\n","        \n","    # when feeding forward an input\n","    def forward(self, x):\n","        # for each head in the heads of the model, we pass the input to each head, then we concatenate the results of heads on the last dimension\n","        out = torch.cat([h(x) for h in self.heads], dim = -1)\n","        \n","        # then we apply the last linear layer of projection and then dropout 20% of the neurons\n","        out = self.dropout(self.proj(out))\n","        \n","        # then return the output\n","        return out"]},{"cell_type":"code","execution_count":95,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:55.569399Z","iopub.status.busy":"2023-12-28T23:55:55.568664Z","iopub.status.idle":"2023-12-28T23:55:55.576098Z","shell.execute_reply":"2023-12-28T23:55:55.575111Z","shell.execute_reply.started":"2023-12-28T23:55:55.569366Z"},"trusted":true},"outputs":[],"source":["# feeding forward input\n","\n","class FeedForward(nn.Module):\n","    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n","    \n","    # on initialization\n","    def __init__(self, n_embed):\n","        \n","        # call the parent function intialization\n","        super().__init__()\n","        \n","        # the net attribute will init the network as a sequential\n","        self.net = nn.Sequential(\n","            \n","            # linear layer that takes n_embed and gives 4 * n_embed\n","            nn.Linear(n_embed, 4 * n_embed),\n","            \n","            # relu non linearity\n","            nn.ReLU(),\n","            \n","            # linear layer that takes 4 * n_embed and gives n_embed\n","            nn.Linear(4 * n_embed, n_embed),\n","            \n","            # dropout that drops 20% of neurons\n","            nn.Dropout(dropout),\n","        )\n","        \n","    # when feeding input forward\n","    def forward(self, x):\n","        \n","        # pass the input into the network\n","        return self.net(x)"]},{"cell_type":"code","execution_count":96,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:56.002886Z","iopub.status.busy":"2023-12-28T23:55:56.002234Z","iopub.status.idle":"2023-12-28T23:55:56.010954Z","shell.execute_reply":"2023-12-28T23:55:56.009932Z","shell.execute_reply.started":"2023-12-28T23:55:56.002851Z"},"trusted":true},"outputs":[],"source":["# a block within the network\n","\n","class Block(nn.Module):\n","    \"\"\" Transformer block: communication followed by computation \"\"\"\n","    \n","    # on intialization\n","    def __init__(self, n_embed, n_head):\n","        \n","        # call the parent function initialization\n","        super().__init__()\n","        \n","        # the head size will be the number of embeddings divided by the number of heads, it will be split among each head\n","        head_size = n_embed // n_head\n","        \n","        # the self attention attribute will be the the multi head attention part which will take in the number of heads and the size of each head\n","        self.sa = MultiHeadAttention(n_head, head_size)\n","        \n","        # the feed forward part will feed forward the input\n","        self.ffwd = FeedForward(n_embed)\n","        \n","        # the first layer norm applied (layer norm is just batch norm excpet instead of normalizing along the dimension of each batch, we normalize along the dimension of the layer)\n","        self.ln1 = nn.LayerNorm(n_embed)\n","        \n","        # the second layer norm applied\n","        self.ln2 = nn.LayerNorm(n_embed)\n","        \n","    # when feeding forward input to the block\n","    def forward(self, x):\n","        \n","        # residual connections are like a tree where the model goes from the input to the output along a big tall tree trunk but the residual connections are where the model branches out performs some computation and then rergroups with the trunk and branches back into it\n","        \n","        # we add residual connections where the model is able to normalize the layer, and then perform the multiheaded self attention and based on this computation add it back into the calculations\n","        x = x + self.sa(self.ln1(x))\n","        \n","        # there is a second residual connection where the model normalizes the layer, then feeds forward some linear layers, relu, and dropout layer, then adds back its calculations to the main branch\n","        x = x + self.ffwd(self.ln2(x))\n","        \n","        # return the ouput of computation\n","        return x"]},{"cell_type":"code","execution_count":97,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:56.370639Z","iopub.status.busy":"2023-12-28T23:55:56.369872Z","iopub.status.idle":"2023-12-28T23:55:56.387780Z","shell.execute_reply":"2023-12-28T23:55:56.386705Z","shell.execute_reply.started":"2023-12-28T23:55:56.370604Z"},"trusted":true},"outputs":[],"source":["# the full gpt language model with all its parts \n","\n","class GPTLanguageModel(nn.Module):\n","    \n","    # on initialization\n","    def __init__(self):\n","        \n","        # call parent initialization function\n","        super().__init__()\n","        \n","        # make the embedding table for each token (vocabulary word) with the number of embeddings requested\n","        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n","        \n","        # make an embedding table for each position in the block_size with the number of embeddings for each position\n","        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n","        \n","        # we want to make n_layer number of blocks with a number of embeddings and number of heads, and we want to contain this list within a sequential module\n","        self.blocks = nn.Sequential(*[Block(n_embed, n_head = n_head) for _ in range(n_layer)])\n","        \n","        # the ln_f attribute will be our layer norm\n","        self.ln_f = nn.LayerNorm(n_embed)\n","        \n","        # the lm_head attribute will be our final linear layer that takes in n_embed as input and generates logits of vocab_size as expected\n","        self.lm_head = nn.Linear(n_embed, vocab_size)\n","        \n","        # then we apply to the model the specific intialization of weights\n","        self.apply(self._init_weights)\n","        \n","    # intialization of weights\n","    def _init_weights(self, module):\n","        \n","        # if the module we are looking at is a linear layer\n","        if isinstance(module, nn.Linear):\n","            \n","            # we want to initialize its weights as a normal distribution of mean 0 and standard deviation 0.02 (this is probably because the network is so large and senstitive that we want very small values as these will blow up inevitably as they work their way through the network)\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","            \n","            # if this linear layer has bias\n","            if module.bias is not None:\n","                \n","                # we want to initialize the bias as all zeroes\n","                torch.nn.init.zeros_(module.bias)\n","                \n","        # however if this is an embedding lookup table\n","        elif isinstance(module, nn.Embedding):\n","            # we want the weights of the lookup table to be drawn from a normal distribute with mean 0 and standard deviation 0.02\n","            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n","            \n","    # when feeding input forward into the model\n","    def forward(self, idx, targets = None):\n","        \n","        # we expect to receive our input in a 2D form of bytes by time\n","        B, T = idx.shape\n","        \n","        # we will get the token embeddings from the token embedding table given these indices\n","        tok_emb = self.token_embedding_table(idx)\n","        \n","        # we will get the position embeddings from the position embedding table by indexing into the table by position in the sequence\n","        pos_emb = self.position_embedding_table(torch.arange(T, device = device))\n","        \n","        # we will start off as a combination of token embeddings and position embeddings\n","        x = tok_emb + pos_emb\n","        \n","        # then we will apply the multiple blocks onto this initial input\n","        x = self.blocks(x)\n","        \n","        # then we will apply a layer norm before we make our prediction\n","        x = self.ln_f(x)\n","        \n","        # then we will pull logits out of our final linear layer's conversion of this input into a vocab_size probability distribution\n","        logits = self.lm_head(x)\n","        \n","        # if there are no targets given to compare to\n","        if targets is None:\n","            # then the loss will be none\n","            loss = None\n","            \n","        # however, if we are given targets to measure our accuracy relative to\n","        else:\n","            \n","            # we take the three dimensions of the logits\n","            B, T, C = logits.shape\n","            \n","            # merge the bytes (batches) and time (positions) dimension of the logits\n","            logits = logits.view(B * T, C)\n","            \n","            # merge the bytes and time dimensions of the targets\n","            targets = targets.view(B * T)\n","            \n","            # and then push them through the cross_entropy loss function to get the loss\n","            loss = F.cross_entropy(logits, targets)\n","            \n","        # then we return the logits and loss generated whether there were targets or not\n","        return logits, loss\n","    \n","    # when making generations\n","    def generate(self, idx, max_new_tokens):\n","        \n","        # we want to make max_new_tokens character generations\n","        for _ in range(max_new_tokens):\n","            \n","            # crop idx to be the last block_size tokens of input\n","            idx_cond = idx[:, -block_size:]\n","            \n","            # then we pass this input into the model which outputs logits and loss (in this case loss is 0 since we have no targets to compare to)\n","            logits, loss = self(idx_cond)\n","            \n","            # then we are only interested in the last instance of time of generation so logits goes from (B, T, C) to (B, C) as we pulled out the last instance of time\n","            logits = logits[:, -1, :]\n","            \n","            # then we take the logits and exponetiate them and normalize them into a probability distribution along the last dimension C (the channels)\n","            probs = F.softmax(logits, dim = -1)\n","            \n","            # then we get the index of the next character predicted by the model by taking the probability distribution generated by the model and using it to sample one character\n","            idx_next = torch.multinomial(probs, num_samples = 1)\n","            \n","            # then we add the new index to our list of indices for us to decode the generation of the model into string text\n","            idx = torch.cat((idx, idx_next), dim = 1)\n","            \n","        # we return the generation of the model\n","        return idx"]},{"cell_type":"code","execution_count":98,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:56.876521Z","iopub.status.busy":"2023-12-28T23:55:56.875824Z","iopub.status.idle":"2023-12-28T23:55:57.095855Z","shell.execute_reply":"2023-12-28T23:55:57.094894Z","shell.execute_reply.started":"2023-12-28T23:55:56.876480Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10.788929 million parameters\n"]}],"source":["# model time!!\n","\n","# initialize the GPTLanguageModel\n","model = GPTLanguageModel()\n","\n","# move the model to the device we wish to train on\n","model = model.to(device)\n","\n","# get the number of parameters by looping through each parameter in the model parameters and getting the number of elements in those parameters and taking the sum of those, dividing by 1e-6 to get in units of millions\n","print(sum(p.numel() for p in model.parameters())/1e6, 'million parameters')\n","\n","# init the optimizer as AdamW and passing in the model parameters to be updated along with the learning rate of updating them\n","optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"]},{"cell_type":"code","execution_count":99,"metadata":{"execution":{"iopub.execute_input":"2023-12-28T23:55:58.854148Z","iopub.status.busy":"2023-12-28T23:55:58.853764Z","iopub.status.idle":"2023-12-29T00:25:57.024302Z","shell.execute_reply":"2023-12-29T00:25:57.023338Z","shell.execute_reply.started":"2023-12-28T23:55:58.854115Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.2669, val loss 4.2666\n","step 500: train loss 1.7948, val loss 1.9358\n","step 1000: train loss 1.4007, val loss 1.6216\n","step 1500: train loss 1.2764, val loss 1.5282\n","step 2000: train loss 1.1932, val loss 1.4974\n","step 2500: train loss 1.1275, val loss 1.4875\n","step 3000: train loss 1.0683, val loss 1.4856\n","step 3500: train loss 1.0195, val loss 1.5105\n","step 4000: train loss 0.9635, val loss 1.5149\n","step 4500: train loss 0.9110, val loss 1.5492\n","step 4999: train loss 0.8566, val loss 1.5787\n"]}],"source":["# training time!!!\n","\n","# for each iteration of training\n","for iter in range(max_iters):\n","    \n","    # if the iteration is one that falls on the evaluation of training interval\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        \n","        # we want to get the current loss on the validation and training set\n","        losses = estimate_loss()\n","        \n","        # then we want to display the current training and validation loss at this iteration\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","    # we get a batch to train on from the training dataset\n","    xb, yb = get_batch('train')\n","    \n","    # we get logits from the model on this batch\n","    logits, loss = m(xb, yb)\n","    \n","    # we reset the gradients on the optimizer and set them to None because this is most efficient\n","    optimizer.zero_grad(set_to_none = True)\n","    \n","    # then we back propagate and calculate gradients for the parameters to descend the loss function\n","    loss.backward()\n","    \n","    # then we take a step in the right direction for the model parameters\n","    optimizer.step()"]},{"cell_type":"code","execution_count":100,"metadata":{"execution":{"iopub.execute_input":"2023-12-29T00:26:00.738015Z","iopub.status.busy":"2023-12-29T00:26:00.737616Z","iopub.status.idle":"2023-12-29T00:26:09.623803Z","shell.execute_reply":"2023-12-29T00:26:09.622750Z","shell.execute_reply.started":"2023-12-29T00:26:00.737983Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","To where I live saway? O do I swell thou wilt,\n","To show a tosse strawing become of mine hours: I\n","What doth nothing would said it were, be not sworn well\n","From which proter off, and justice comes.\n","\n","SICINIUS:\n","Have we a sweary wife\n","That whereof? we have said here comes\n","With our fruit-pilegr wrection laid,\n","And by the weeds of whoxes her from them\n","The father's friends, rends to Time, and we two\n","The grates are on it.\n","\n","First Soldier:\n","Have you slipted with the war fair?\n","\n","ARTCUS:\n","Upon the tyrannous and Rom\n"]}],"source":["# this context will be dataype long and want to move it to the device we wish to work on\n","context = torch.zeros((1, 1), dtype = torch.long, device = device)\n","\n","# then we want to generate 500 characters given the context using the model specifically on the device we want to work on and then we want to pull out the indexes generated, convert them to a list of integers, and then give them to the decoder to decode so we can print them\n","print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))"]},{"cell_type":"code","execution_count":101,"metadata":{"execution":{"iopub.execute_input":"2023-12-29T00:29:08.886362Z","iopub.status.busy":"2023-12-29T00:29:08.885956Z","iopub.status.idle":"2023-12-29T00:29:09.108054Z","shell.execute_reply":"2023-12-29T00:29:09.107050Z","shell.execute_reply.started":"2023-12-29T00:29:08.886332Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["10.788929 million parameters\n"]}],"source":["# i would like to compare this model trained on 5000 steps with loss 0.8566 and val 1.5787 to the model trained on 3000 steps with loss 1.0683 and val 1.4856. The question I want to answer here is whether it is more important to have a lower training loss for text generation or a lower val loss\n","\n","# hyperparameters of the transformer model\n","batch_size = 64 # how many independent sequences to train on in parallel\n","block_size = 256 # maximum size of input context and previous characters the network can use to predict the next character\n","max_iters = 3000 # how many iterations of training\n","eval_interval = 500 # how often to display the current loss in training\n","learning_rate = 3e-4 # the rate of parameter updating\n","device = 'cuda' if torch.cuda.is_available() else 'cpu' # deciding which device to train on\n","eval_iters = 200\n","n_embed = 384 # how many dimensions the network has to work with for embedding the vocabulary\n","n_head = 6 # the number of heads to use to extract features from the input data in order to better be able to understand what came before in order to more effectively predict what should come next\n","n_layer = 6 # the number of linear layers applied throughout the model to help the model not only transform the input into something it can understand, but actually be able to let the information marinate and go through some internal processing before the output\n","dropout = 0.2 # the proportion of neurons to shut off randomly during each iteration in order to prevent overtraining and overfitting to the dataset to make sure that the network is actually able to perform and predict well and not just simply memorize the data\n","\n","# initialize the GPTLanguageModel\n","model = GPTLanguageModel()\n","\n","# move the model to the device we wish to train on\n","m = model.to(device)\n","\n","# get the number of parameters by looping through each parameter in the model parameters and getting the number of elements in those parameters and taking the sum of those, dividing by 1e-6 to get in units of millions\n","print(sum(p.numel() for p in m.parameters())/1e6, 'million parameters')\n","\n","# init the optimizer as AdamW and passing in the model parameters to be updated along with the learning rate of updating them\n","optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)"]},{"cell_type":"code","execution_count":102,"metadata":{"execution":{"iopub.execute_input":"2023-12-29T00:29:10.624675Z","iopub.status.busy":"2023-12-29T00:29:10.624064Z","iopub.status.idle":"2023-12-29T00:47:22.421715Z","shell.execute_reply":"2023-12-29T00:47:22.420837Z","shell.execute_reply.started":"2023-12-29T00:29:10.624638Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["step 0: train loss 4.1869, val loss 4.1959\n","step 500: train loss 1.7744, val loss 1.9227\n","step 1000: train loss 1.4043, val loss 1.6082\n","step 1500: train loss 1.2732, val loss 1.5263\n","step 2000: train loss 1.1967, val loss 1.5059\n","step 2500: train loss 1.1352, val loss 1.4893\n","step 2999: train loss 1.0780, val loss 1.4891\n"]}],"source":["# for each iteration of training\n","for iter in range(max_iters):\n","    \n","    # if the iteration is one that falls on the evaluation of training interval\n","    if iter % eval_interval == 0 or iter == max_iters - 1:\n","        \n","        # we want to get the current loss on the validation and training set\n","        losses = estimate_loss()\n","        \n","        # then we want to display the current training and validation loss at this iteration\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","        \n","    # we get a batch to train on from the training dataset\n","    xb, yb = get_batch('train')\n","    \n","    # we get logits from the model on this batch\n","    logits, loss = m(xb, yb)\n","    \n","    # we reset the gradients on the optimizer and set them to None because this is most efficient\n","    optimizer.zero_grad(set_to_none = True)\n","    \n","    # then we back propagate and calculate gradients for the parameters to descend the loss function\n","    loss.backward()\n","    \n","    # then we take a step in the right direction for the model parameters\n","    optimizer.step()"]},{"cell_type":"code","execution_count":103,"metadata":{"execution":{"iopub.execute_input":"2023-12-29T00:47:22.424551Z","iopub.status.busy":"2023-12-29T00:47:22.423647Z","iopub.status.idle":"2023-12-29T00:47:31.412141Z","shell.execute_reply":"2023-12-29T00:47:31.411166Z","shell.execute_reply.started":"2023-12-29T00:47:22.424506Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","TAO, but thy brother gautes; 'tis fast thou did'st:\n","Thy heavensings! while the time in thy face, like trum!\n","Wroum like to have an head, apotIter,\n","So rudly, quart true! Who, senses! the wormst be\n","Might the book, some ready, to walk, and put I dark thee,\n","More-than chased with thee one:\n","Good with the son of station twen times of the low,\n","Show thy breath and these triner of time,\n","But in thy place kingdom out on't.\n","\n","JULIET:\n","And were the namedples that ames-day in my chief:\n","What, you deserts doth sple\n"]}],"source":["# this context will be dataype long and want to move it to the device we wish to work on\n","context = torch.zeros((1, 1), dtype = torch.long, device = device)\n","\n","# then we want to generate 500 characters given the context using the model specifically on the device we want to work on and then we want to pull out the indexes generated, convert them to a list of integers, and then give them to the decoder to decode so we can print them\n","print(decode(m.generate(context, max_new_tokens = 500)[0].tolist()))"]},{"cell_type":"markdown","metadata":{},"source":["**5000 steps, train loss 0.8566, val loss 1.5787 model generation:**\n","\n","*To where I live saway? O do I swell thou wilt,\n","To show a tosse strawing become of mine hours: I\n","What doth nothing would said it were, be not sworn well\n","From which proter off, and justice comes.*\n","\n","*SICINIUS:\n","Have we a sweary wife\n","That whereof? we have said here comes\n","With our fruit-pilegr wrection laid,\n","And by the weeds of whoxes her from them\n","The father's friends, rends to Time, and we two\n","The grates are on it.*\n","\n","*First Soldier:\n","Have you slipted with the war fair?*\n","\n","*ARTCUS:\n","Upon the tyrannous and Rom*\n","\n","\n","**3000 steps, train loss 1.0780, val loss 1.4891 model generation:**\n","\n","*TAO, but thy brother gautes; 'tis fast thou did'st:\n","Thy heavensings! while the time in thy face, like trum!\n","Wroum like to have an head, apotIter,\n","So rudly, quart true! Who, senses! the wormst be\n","Might the book, some ready, to walk, and put I dark thee,\n","More-than chased with thee one:\n","Good with the son of station twen times of the low,\n","Show thy breath and these triner of time,\n","But in thy place kingdom out on't.*\n","\n","*JULIET:\n","And were the namedples that ames-day in my chief:\n","What, you deserts doth sple*\n","\n","\n","Personally, I would say the first one with the lower train loss and slightly higher val loss did better, so perhaps in the case of random text generation, the lower train loss seems to do better\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4234429,"sourceId":7299397,"sourceType":"datasetVersion"}],"dockerImageVersionId":30627,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
