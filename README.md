This repo contains my very first experience with building a GPT and training it on a text dataset.

A couple comments and notes I would like to make:
- I am very thankful to Andrej Karpathy for his <a href="https://www.youtube.com/watch?v=VMj-3S1tku0&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ">youtube lecture series</a>. Thanks to his work, I was able to very quickly learn and understand how neural networks operate and his lectures enabled me to advance this fast.
- This is, for the most part, basically a carbon copy of the code written by Karpathy in the lecture.
- However, because of the instructiveness of the lectures, I was able to very heavily annotate this notebook, line by line, to explain how the GPT architecture works and thrives on multi head attention
- I am uploading this to github about 3 months after I initially worked on this notebook.
- I have not heavily scrutizined the contents of this notebook so there may be errors in the annotations.
- I am uploading this as a marker in my progress.
- My progress has slowed down since I finished all the lectures in the lecture series, only jumpstarting back up after the most recent tokenizer lecture addition. I am hoping that more lectures are uploaded because I learn the most from them. In the meantime, I am left to learning more by reading research papers and working on mini projects. These are not nearly as helpful as the lecture series has been for me but I must do what I can to continue progressing.
